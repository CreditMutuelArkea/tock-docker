# WARNING to run this you need to ensure that vm.max_map_count is correctly configured
# See https://opensearch.org/docs/latest/install-and-configure/install-opensearch/docker#important-host-settings
# We recommande having at least 40Gb free space on your disk to prevent this error : https://stackoverflow.com/a/67767478
#   If you have less and something goes wrong at the cluster boot check that you don't have any logs "high disk watermark [90%] exceeded"
#   if so make some free space a few Gb until this error desapear.
version: '3'

services:

  mongo:
    image: "${PLATFORM}mongo:${MONGO}"
    volumes:
      - tockmongo:/data/db
    ports:
      - "27017:27017"
    command: --bind_ip_all --port 27017 --replSet "tock"

  mongo2:
    image: "${PLATFORM}mongo:${MONGO}"
    depends_on:
      - mongo
    volumes:
      - tockmongo2:/data/db
    ports:
      - "27018:27018"
    command: --bind_ip_all --port 27018 --replSet "tock"

  mongo3:
    image: "${PLATFORM}mongo:${MONGO}"
    depends_on:
      - mongo
      - mongo2
    volumes:
      - tockmongo3:/data/db
    ports:
      - "27019:27019"
    command: --bind_ip_all --port 27019 --replSet "tock"

  mongo-setup:
    container_name: "mongo-setup"
    image: "${PLATFORM}mongo:${MONGO}"
    depends_on:
      - "mongo"
      - "mongo2"
      - "mongo3"
    links:
      - mongo:mongo
      - mongo2:mongo2
      - mongo3:mongo3
    volumes:
      - ./scripts:/scripts
    environment:
      - MONGO1=mongo
      - MONGO2=mongo2
      - MONGO3=mongo3
      - RS=tock
    entrypoint: [ "/scripts/setup.sh" ]

  build_worker:
    image: "tock/build_worker:${TAG}"
    depends_on:
      - mongo
      - mongo2
      - mongo3
    environment:
      - tock_mongo_url=mongodb://mongo:27017,mongo2:27018,mongo3:27019/?replicaSet=tock
      - tock_env=prod
      - JAVA_ARGS=-Xmx1g -XX:MaxMetaspaceSize=256m
    #  - tock_default_log_level=warn
    #  - tock_service_log_level=info
    #  - tock_database_log_level=warn

  duckling:
    image: "tock/duckling:${TAG}"
    environment:
      - tock_env=prod
      # - tock_default_log_level=warn
      # - tock_service_log_level=info
      # - tock_database_log_level=warn
    expose:
      - "8080"

  kotlin_compiler:
    image: "tock/kotlin_compiler:${TAG}"
    environment:
      - tock_env=prod
      - tock_kotlin_compiler_classpath=/maven
#      - tock_default_log_level=warn
#      - tock_service_log_level=info
#      - tock_database_log_level=warn
    expose:
      - "8080"

  admin_web:
    image: "tock/bot_admin:${TAG}"
    depends_on:
      - mongo
      - mongo2
      - mongo3
      - duckling
      - kotlin_compiler
    environment:
      - no_proxy=duckling,bot_api,kotlin_compiler
      - NO_PROXY=duckling,bot_api,kotlin_compiler
      - tock_mongo_url=mongodb://mongo:27017,mongo2:27018,mongo3:27019/?replicaSet=tock
      - nlp_duckling_url=http://duckling:8080
      - tock_env=prod
      - tock_bot_admin_rest_default_base_url=http://bot_api:8080
      - tock_bot_compiler_service_url=http://kotlin_compiler:8080
      - tock_configuration_bot_default_base_url=http://bot_api:8080
      - tock_llm_orchestrator_server_url=http://llm_orchestrator-server:8000
      - tock_llm_orchestrator_client_request_timeout_ms=55000
      - tock_gen_ai_orchestrator_technical_error=Technical error :( sorry!
      #remove this in production
      - tock_https_env=false
      - botadminverticle_body_limit=-1
#      - tock_default_log_level=warn
#      - tock_service_log_level=info
#      - tock_database_log_level=warn
    ports:
      - "80:8080"

  nlp_api:
    image: "tock/nlp_api:${TAG}"
    depends_on:
      - mongo
      - mongo2
      - mongo3
      - duckling
    environment:
      - no_proxy=duckling #Prevent traffic from using environment proxy
      - NO_PROXY=duckling
      - tock_mongo_url=mongodb://mongo:27017,mongo2:27018,mongo3:27019/?replicaSet=tock
      - nlp_duckling_url=http://duckling:8080
      - tock_env=prod
      - tock_web_use_default_cors_handler=true
      - tock_web_use_default_cors_handler_with_credentials=false
      - tock_web_use_default_cors_handler_url=*
#      - tock_default_log_level=warn
#      - tock_service_log_level=info
#      - tock_database_log_level=warn
    ports:
      - "8888:8080"

  bot_api:
    image: "tock/bot_api:${TAG}"
    depends_on:
      - mongo
      - mongo2
      - mongo3
      - nlp_api
    environment:
      - no_proxy=nlp_api #Prevent traffic from using environment proxy
      - NO_PROXY=nlp_api
      - tock_mongo_url=mongodb://mongo:27017,mongo2:27018,mongo3:27019/?replicaSet=tock
      - tock_nlp_service_url=http://nlp_api:8080
      - tock_env=integ
      - tock_websocket_enabled=true
      - tock_llm_orchestrator_server_url=http://llm_orchestrator-server:8000
      - tock_llm_orchestrator_client_request_timeout_ms=55000
      - tock_gen_ai_orchestrator_technical_error=Technical error :( sorry!
#      - tock_default_log_level=warn
#      - tock_service_log_level=info
#      - tock_database_log_level=warn
    ports:
      - "8080:8080"

  opensearch-node1:
    image: "${PLATFORM}opensearchproject/opensearch:${OPENSEARCH}"
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # maximum number of open files for the OpenSearch user, set to at least 65536 on modern systems
        hard: 65536
    volumes:
      - opensearch-data1:/usr/share/opensearch/data
    ports:
      - 9200:9200
      - 9600:9600 # required for Performance Analyzer

  opensearch-node2:
    image: "${PLATFORM}opensearchproject/opensearch:${OPENSEARCH}"
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node2
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data2:/usr/share/opensearch/data

  opensearch-dashboards:
    image: "${PLATFORM}opensearchproject/opensearch-dashboards:${OPENSEARCH}"
    ports:
      - 5601:5601
    expose:
      - "5601"
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch-node1:9200","https://opensearch-node2:9200"]'

  llm_orchestrator-server:
    image: "${PLATFORM}tock/llm-orchestrator-server:${TAG}"
    ports:
      - "8000:8000"
    environment:
      tock_gen_ai_orchestrator_application_environment: DEV
      tock_gen_ai_orchestrator_open_search_host: opensearch-node1
      tock_gen_ai_orchestrator_open_search_port: 9200
      tock_gen_ai_orchestrator_open_search_pwd: admin
      tock_gen_ai_orchestrator_open_search_user: admin


volumes:
  tockmongo:
  tockmongo2:
  tockmongo3:

  opensearch-data1:
  opensearch-data2:
